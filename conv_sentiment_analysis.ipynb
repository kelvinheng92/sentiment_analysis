{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"conv_sentiment_analysis.ipynb","provenance":[{"file_id":"https://github.com/kelvinheng92/sentiment_analysis/blob/master/baseline.ipynb","timestamp":1589457930206},{"file_id":"https://github.com/kelvinheng92/sentiment_analysis/blob/master/baseline.ipynb","timestamp":1589417551888}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"D0fSKmcvTUU3","colab_type":"text"},"source":["# Data Preparation"]},{"cell_type":"code","metadata":{"id":"L1xB2xwRTZKg","colab_type":"code","colab":{}},"source":["import torch\n","from torchtext import data, datasets\n","import random\n","\n","SEED = 1992\n","\n","torch.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True\n","\n","TEXT = data.Field(tokenize = 'spacy', batch_first=True)\n","LABEL = data.LabelField(dtype = torch.float)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jxyR5l5oTjAh","colab_type":"code","outputId":"d12cf08d-36cd-4b57-c942-5fe0d9b0e9ef","executionInfo":{"status":"ok","timestamp":1590543088516,"user_tz":-480,"elapsed":101772,"user":{"displayName":"Kelvin Heng","photoUrl":"","userId":"16189608668730772404"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n","valid_data, test_data = test_data.split(split_ratio = 0.5, random_state = random.seed(SEED)) #Further split test set into validation and test set"],"execution_count":2,"outputs":[{"output_type":"stream","text":["\raclImdb_v1.tar.gz:   0%|          | 0.00/84.1M [00:00<?, ?B/s]"],"name":"stderr"},{"output_type":"stream","text":["downloading aclImdb_v1.tar.gz\n"],"name":"stdout"},{"output_type":"stream","text":["aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:03<00:00, 21.4MB/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"ddKoNplZUY7Q","colab_type":"code","outputId":"537a9e88-759d-496e-9163-4475999c7c3a","executionInfo":{"status":"ok","timestamp":1590543624072,"user_tz":-480,"elapsed":847,"user":{"displayName":"Kelvin Heng","photoUrl":"","userId":"16189608668730772404"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["print(f'Number of training examples: {len(train_data)}')\n","print(f'Number of validation examples: {len(valid_data)}')\n","print(f'Number of testing examples: {len(test_data)}')"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Number of training examples: 25000\n","Number of validation examples: 12500\n","Number of testing examples: 12500\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IhV_vj1GUuZZ","colab_type":"code","outputId":"26fb5f16-f219-43e0-8cb7-3132a7c4ddb4","executionInfo":{"status":"ok","timestamp":1590543624934,"user_tz":-480,"elapsed":707,"user":{"displayName":"Kelvin Heng","photoUrl":"","userId":"16189608668730772404"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["print(vars(train_data.examples[0]))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["{'text': ['Loosely', 'based', 'on', 'the', 'James', 'J', 'Corbett', 'biography', '\"', 'The', 'Roar', 'Of', 'The', 'Crowd', '\"', ',', 'Gentleman', 'Jim', 'is', 'a', 'wonderfully', 'breezy', 'picture', 'that', 'perfectly', 'encapsulates', 'not', 'only', 'the', 'rise', 'of', 'the', 'pugilistic', 'prancer', 'that', 'was', 'Corbett', ',', 'but', 'also', 'the', 'wind', 'of', 'change', 'as', 'regards', 'the', 'sport', 'of', 'boxing', 'circa', 'the', '1890s.<br', '/><br', '/>The', 'story', 'follows', 'Corbett', '{', 'a', 'perfectly', 'casted', 'Errol', 'Flynn', '}', 'from', 'his', 'humble', 'beginnings', 'as', 'a', 'bank', 'teller', 'in', 'San', 'Fransico', ',', 'thru', 'to', 'a', 'chance', 'fight', 'with', 'an', 'ex', 'boxing', 'champion', 'that', 'eventually', 'leads', 'to', 'him', 'fighting', 'the', 'fearsome', 'heavyweight', 'champion', 'of', 'the', 'world', ',', 'John', 'L', 'Sullivan', '{', 'beefcake', 'personified', 'delightfully', 'by', 'Ward', 'Bond', '}', '.', 'Not', 'all', 'the', 'fights', 'are', 'in', 'the', 'ring', 'tho', ',', 'and', 'it', \"'s\", 'all', 'the', 'spin', 'off', 'vignettes', 'in', 'Corbett', \"'s\", 'life', 'that', 'makes', 'this', 'a', 'grand', 'entertaining', 'picture', '.', 'There', 'are', 'class', 'issues', 'to', 'overcome', 'here', '{', 'perfectly', 'played', 'out', 'as', 'fellow', 'club', 'members', 'pay', 'to', 'have', 'him', 'knocked', 'down', 'a', 'peg', 'or', 'two', '}', ',', 'and', 'Corbett', 'has', 'to', 'not', 'only', 'fight', 'to', 'get', 'respect', 'from', 'his', 'so', 'called', 'peers', ',', 'but', 'he', 'must', 'also', 'overcome', 'his', 'ego', 'as', 'it', 'grows', 'as', 'briskly', 'as', 'his', 'reputation', 'does', '.', 'Along', 'with', 'the', 'quite', 'wonderful', 'Corbett', 'family', ',', 'and', 'all', 'their', 'stoic', 'humorous', 'support', ',', 'Corbett', \"'s\", 'journey', 'is', 'as', 'enthralling', 'as', 'it', 'is', 'joyous', ',', 'yet', 'as', 'brash', 'and', 'as', 'bold', 'as', 'he', 'is', ',', 'he', 'is', 'a', 'very', 'likable', 'character', ',', 'and', 'it', \"'s\", 'a', 'character', 'that', 'befits', 'the', 'tagged', 'moniker', 'he', 'got', 'of', 'Gentleman', 'Jim.<br', '/><br', '/>The', 'film', 'never', 'sags', 'for', 'one', 'moment', ',', 'and', 'it', \"'s\", 'a', 'testament', 'to', 'director', 'Raoul', 'Walsh', 'that', 'although', 'we', 'are', 'eagerly', 'awaiting', 'the', 'final', 'fight', ',', 'the', 'outer', 'ring', 'goings', 'on', 'are', 'keeping', 'us', 'firmly', 'entertained', ',', 'not', 'even', 'the', 'love', 'interest', 'sub', 'plot', 'hurts', 'this', 'picture', '{', 'thank', 'you', 'Alexis', 'Smith', '}', '.', 'The', 'fight', 'sequences', 'stand', 'up', 'really', 'well', ',', 'and', 'they', 'perfectly', 'show', 'just', 'how', 'Corbett', 'became', 'the', 'champ', 'he', 'was', ',', 'his', 'brand', 'of', 'dancing', 'rings', 'round', 'slugger', 'fighters', 'is', 'now', 'firmly', 'placed', 'in', 'boxing', 'history', '.', 'As', 'the', 'final', 'reel', 'rolls', 'we', 'all', 'come', 'down', 'to', 'earth', 'as', 'an', 'after', 'fight', 'meeting', 'between', 'Sullivan', 'and', 'Corbett', 'puts', 'all', 'the', 'brutality', 'into', 'context', ',', 'and', 'it', \"'s\", 'here', 'where', 'humility', 'and', 'humbleness', 'becomes', 'the', 'outright', 'winner', ',', 'and', 'as', 'far', 'as', 'this', 'viewer', 'goes', '..............', 'it', 'will', 'do', 'for', 'me', 'to', 'be', 'sure', 'to', 'be', 'sure', ',', '9/10', 'for', 'a', 'truly', 'wonderful', 'picture', '.'], 'label': 'pos'}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DW8OoSamoN_5","colab_type":"code","outputId":"6bcbc6f2-777f-4574-fc92-c29e61e57c6c","executionInfo":{"status":"ok","timestamp":1590544062079,"user_tz":-480,"elapsed":436898,"user":{"displayName":"Kelvin Heng","photoUrl":"","userId":"16189608668730772404"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["MAX_VOCAB_SIZE = 25_000\n","TEXT.build_vocab(train_data,\n","                 max_size = MAX_VOCAB_SIZE,\n","                 vectors = \"glove.6B.100d\",\n","                 unk_init = torch.Tensor.normal_)\n","LABEL.build_vocab(train_data)"],"execution_count":8,"outputs":[{"output_type":"stream","text":[".vector_cache/glove.6B.zip: 862MB [06:28, 2.22MB/s]                           \n","100%|█████████▉| 399597/400000 [00:15<00:00, 26159.56it/s]"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"xKpI5orppdF4","colab_type":"code","outputId":"d36fa9c4-a5b7-4685-f905-192b73aa04e1","executionInfo":{"status":"ok","timestamp":1590544062080,"user_tz":-480,"elapsed":430850,"user":{"displayName":"Kelvin Heng","photoUrl":"","userId":"16189608668730772404"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["vars(LABEL.vocab)"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'freqs': Counter({'neg': 12500, 'pos': 12500}),\n"," 'itos': ['neg', 'pos'],\n"," 'stoi': defaultdict(<function torchtext.vocab._default_unk_index>,\n","             {'neg': 0, 'pos': 1}),\n"," 'vectors': None}"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"_zHC1He0oqbn","colab_type":"code","outputId":"6e85aef2-67f1-43b1-8283-731fd7e90091","executionInfo":{"status":"ok","timestamp":1590544062080,"user_tz":-480,"elapsed":429124,"user":{"displayName":"Kelvin Heng","photoUrl":"","userId":"16189608668730772404"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["print(f'There are {len(TEXT.vocab)} unique tokens in TEXT vocabulary')\n","print(f'There are {len(LABEL.vocab)} unique tokens in LABEL vocabulary')"],"execution_count":10,"outputs":[{"output_type":"stream","text":["There are 25002 unique tokens in TEXT vocabulary\n","There are 2 unique tokens in LABEL vocabulary\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mwaiWFisr1IW","colab_type":"code","outputId":"e82eefb6-18d0-4369-a8bb-bf19504eaf36","executionInfo":{"status":"ok","timestamp":1590544062081,"user_tz":-480,"elapsed":426743,"user":{"displayName":"Kelvin Heng","photoUrl":"","userId":"16189608668730772404"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["print(vars(TEXT.vocab).keys())\n","print(vars(LABEL.vocab).keys())"],"execution_count":11,"outputs":[{"output_type":"stream","text":["dict_keys(['freqs', 'itos', 'stoi', 'vectors'])\n","dict_keys(['freqs', 'itos', 'stoi', 'vectors'])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Oa95Dc4WrQ4l","colab_type":"code","colab":{}},"source":["BATCH_SIZE = 64\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits((train_data, valid_data, test_data),\n","                                                                           batch_size = BATCH_SIZE,\n","                                                                           device = device)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zk5IUombrJRC","colab_type":"text"},"source":["# Model Architecture"]},{"cell_type":"code","metadata":{"id":"V0GhJG2Jt3I_","colab_type":"code","colab":{}},"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class CNN(nn.Module):\n","  def __init__(self, input_dim, emb_dim, num_filters,\n","               filter_sizes, output_dim, dropout, pad_idx):\n","    \n","    super().__init__() # init from super class in nn.module\n","    self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx = pad_idx)\n","\n","    self.convs = nn.ModuleList([nn.Conv2d(in_channels = 1,\n","                                out_channels = num_filters,\n","                                kernel_size = (fs, emb_dim))\n","                                for fs in filter_sizes\n","                                ])\n","\n","    self.fc = nn.Linear(num_filters * len(filter_sizes) , output_dim)\n","    self.dropout = nn.Dropout(dropout)\n","\n","  def forward(self, text):\n","    # text has dimension of (batch_size, sentence_length)\n","    embedding_output = self.embedding(text)\n","    # embedding_output has dimension of (batch_size, sentence_length, emb_dim)\n","    embedding_output = embedding_output.unsqueeze(1)\n","    # embedding_output has dimension of (batch_size, 1, sentence_length, emb_dim)\n","\n","    convs_output = [F.relu(conv(embedding_output).squeeze(3)) for conv in self.convs]\n","    # Each of the tensor in convs_output has dimension of (batch_size, num_filters, sent_len - filter_sizes[N] + 1)\n","\n","    pools_output = [F.max_pool1d(conv_output, kernel_size = conv_output.shape[-1]).squeeze(2) for conv_output in convs_output]\n","    # Each of the tensor in pools_output has dimension of (batch_size, num_filters) after squeezing\n","\n","    concat = torch.cat(pools_output, dim = 1)\n","    concat = self.dropout(concat)\n","    # concat has dimension of (batch_size, num_filters * len(filter_sizes))\n","\n","    fc_output = self.fc(concat)\n","    # fc_output has dimension of (batch_size, output_dim) after fc layer\n","\n","    return fc_output"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9JWdGeNhrE7M","colab_type":"code","colab":{}},"source":["INPUT_DIM = len(TEXT.vocab)\n","EMB_DIM = 100\n","NUM_FILTERS = 100\n","FILTER_SIZES = [3, 4, 5]\n","OUTPUT_DIM = 1\n","DROPOUT = 0.5\n","PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n","\n","model = CNN(INPUT_DIM, EMB_DIM, NUM_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)\n","model = model.to(device)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yZPN1y6es4zE","colab_type":"code","outputId":"01cd826f-cd62-4a6d-d6ac-adf13c952d3c","executionInfo":{"status":"ok","timestamp":1590545908623,"user_tz":-480,"elapsed":724,"user":{"displayName":"Kelvin Heng","photoUrl":"","userId":"16189608668730772404"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'The model has {count_parameters(model):,} trainable parameters')"],"execution_count":47,"outputs":[{"output_type":"stream","text":["The model has 2,620,801 trainable parameters\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UqsidqA3OYG2","colab_type":"code","outputId":"7a7346dd-1553-422f-e2cc-8e0b58c7c1fa","executionInfo":{"status":"ok","timestamp":1590545909058,"user_tz":-480,"elapsed":860,"user":{"displayName":"Kelvin Heng","photoUrl":"","userId":"16189608668730772404"}},"colab":{"base_uri":"https://localhost:8080/","height":170}},"source":["pretrained_embeddings = TEXT.vocab.vectors\n","print(pretrained_embeddings.shape)\n","model.embedding.weight.data.copy_(pretrained_embeddings)"],"execution_count":48,"outputs":[{"output_type":"stream","text":["torch.Size([25002, 100])\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.3358, -0.0554,  0.3294,  ...,  0.8827,  0.0913, -2.2403],\n","        [-0.0548, -0.7422,  0.8423,  ..., -0.6081,  1.1196, -1.0543],\n","        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n","        ...,\n","        [-0.5826, -0.1010,  1.1440,  ...,  0.1084,  0.8885, -0.2234],\n","        [-0.6728,  1.0572,  0.2273,  ..., -0.4935, -1.3922, -1.3847],\n","        [ 0.1171, -1.1903, -1.0085,  ..., -1.1771,  1.2799,  0.5319]],\n","       device='cuda:0')"]},"metadata":{"tags":[]},"execution_count":48}]},{"cell_type":"code","metadata":{"id":"9WoROUbwO4zJ","colab_type":"code","outputId":"cef0278a-3dfd-4773-e93a-4f35d5a27733","executionInfo":{"status":"ok","timestamp":1590545909059,"user_tz":-480,"elapsed":701,"user":{"displayName":"Kelvin Heng","photoUrl":"","userId":"16189608668730772404"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n","model.embedding.weight.data[UNK_IDX] = torch.zeros(EMB_DIM)\n","model.embedding.weight.data[PAD_IDX] = torch.zeros(EMB_DIM)\n","print(model.embedding.weight.data)"],"execution_count":49,"outputs":[{"output_type":"stream","text":["tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n","        ...,\n","        [-0.5826, -0.1010,  1.1440,  ...,  0.1084,  0.8885, -0.2234],\n","        [-0.6728,  1.0572,  0.2273,  ..., -0.4935, -1.3922, -1.3847],\n","        [ 0.1171, -1.1903, -1.0085,  ..., -1.1771,  1.2799,  0.5319]],\n","       device='cuda:0')\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tvB3PqOJtwzj","colab_type":"text"},"source":["# Define loss, metric and optimizer"]},{"cell_type":"code","metadata":{"id":"Q9AbMu09uOGk","colab_type":"code","colab":{}},"source":["import torch.optim as optim\n","\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","criterion = nn.BCEWithLogitsLoss()\n","criterion = criterion.to(device)\n","\n","def binary_accuracy(preds, label):\n","\n","    rounded_preds = torch.round(torch.sigmoid(preds))\n","    correct = (rounded_preds == label).float() \n","    acc = correct.sum() / len(correct)\n","    return acc"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3fvPvuJKv1J0","colab_type":"text"},"source":["# Train Model"]},{"cell_type":"code","metadata":{"id":"0Z4ax_BIv4n0","colab_type":"code","colab":{}},"source":["import time\n","\n","def train_model(model, iterator, optimizer, criterion):\n","  # start of new epoch \n","  start_time = time.time()\n","  epoch_loss = 0\n","  epoch_acc = 0\n","  model.train() # set model to train mdoe to enable dropout and batch norm\n","  batch_size = len(iterator)\n","\n","  for batch in iterator:\n","    optimizer.zero_grad() # set optimizer grad to 0 first\n","    pred = model(batch.text).squeeze(1) # forward prop\n","    loss = criterion(pred, batch.label) # calculate loss \n","    acc = binary_accuracy(pred, batch.label) # calculate metric\n","    loss.backward() # backward prop\n","    optimizer.step() # updates optimizer parameters\n","    epoch_loss += loss.item()\n","    epoch_acc += acc.item()\n","\n","  end_time = time.time()\n","  epoch_time_taken = end_time - start_time\n","\n","  return epoch_loss / batch_size, epoch_acc / batch_size, epoch_time_taken"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ROn2vg4hykAw","colab_type":"code","colab":{}},"source":["from datetime import datetime\n","\n","def evaluate_model(model, iterator, criterion):\n","  # start of new epoch \n","  start_time = time.time()\n","  epoch_loss = 0\n","  epoch_acc = 0\n","  batch_size = len(iterator)\n","  model.eval() # set model to eval mode\n","\n","  for batch in iterator:\n","    pred = model(batch.text).squeeze(1) # forward prop\n","    loss = criterion(pred, batch.label) # calculate loss \n","    acc = binary_accuracy(pred, batch.label) # calculate metric\n","    epoch_loss += loss.item()\n","    epoch_acc += acc.item()\n","\n","  end_time = time.time()\n","  epoch_time_taken = end_time - start_time\n","\n","  return epoch_loss / batch_size, epoch_acc / batch_size, epoch_time_taken"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HqmKMQMp1mV4","colab_type":"code","colab":{}},"source":["import time\n","\n","def convert_time(time_taken):\n","    elapsed_mins = int(time_taken / 60)\n","    elapsed_secs = int(time_taken - (elapsed_mins * 60))\n","    output = f'{elapsed_mins} min and {elapsed_secs} sec'\n","    return output"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZwsXbles0lUZ","colab_type":"code","outputId":"dc864aac-1122-4a10-e2e2-3aba66dbfc4d","executionInfo":{"status":"ok","timestamp":1590546172215,"user_tz":-480,"elapsed":260568,"user":{"displayName":"Kelvin Heng","photoUrl":"","userId":"16189608668730772404"}},"colab":{"base_uri":"https://localhost:8080/","height":527}},"source":["MAX_EPOCHS = 10\n","\n","best_loss = float('inf')\n","for epoch in range(MAX_EPOCHS):\n","  train_loss, train_acc, train_time = train_model(model, train_iterator, optimizer, criterion)\n","  valid_loss, valid_acc, valid_time = evaluate_model(model, valid_iterator, criterion)\n","  print(f'Epoch {epoch} took {convert_time(train_time)} for training and {convert_time(valid_time)} for validation')\n","\n","  if valid_loss < best_loss:\n","    best_loss = valid_loss\n","    torch.save(model.state_dict(), 'model.pt')\n","\n","  print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","  print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n"],"execution_count":54,"outputs":[{"output_type":"stream","text":["Epoch 0 took 0 min and 25 sec for training and 0 min and 1 sec for validation\n","\tTrain Loss: 0.615 | Train Acc: 64.69%\n","\t Val. Loss: 0.428 |  Val. Acc: 81.08%\n","Epoch 1 took 0 min and 24 sec for training and 0 min and 1 sec for validation\n","\tTrain Loss: 0.369 | Train Acc: 83.66%\n","\t Val. Loss: 0.336 |  Val. Acc: 85.34%\n","Epoch 2 took 0 min and 24 sec for training and 0 min and 1 sec for validation\n","\tTrain Loss: 0.266 | Train Acc: 89.24%\n","\t Val. Loss: 0.330 |  Val. Acc: 85.84%\n","Epoch 3 took 0 min and 24 sec for training and 0 min and 1 sec for validation\n","\tTrain Loss: 0.190 | Train Acc: 92.70%\n","\t Val. Loss: 0.328 |  Val. Acc: 86.39%\n","Epoch 4 took 0 min and 25 sec for training and 0 min and 1 sec for validation\n","\tTrain Loss: 0.135 | Train Acc: 95.14%\n","\t Val. Loss: 0.365 |  Val. Acc: 86.03%\n","Epoch 5 took 0 min and 24 sec for training and 0 min and 1 sec for validation\n","\tTrain Loss: 0.097 | Train Acc: 96.59%\n","\t Val. Loss: 0.402 |  Val. Acc: 85.83%\n","Epoch 6 took 0 min and 25 sec for training and 0 min and 1 sec for validation\n","\tTrain Loss: 0.070 | Train Acc: 97.51%\n","\t Val. Loss: 0.448 |  Val. Acc: 85.47%\n","Epoch 7 took 0 min and 24 sec for training and 0 min and 1 sec for validation\n","\tTrain Loss: 0.050 | Train Acc: 98.31%\n","\t Val. Loss: 0.474 |  Val. Acc: 85.88%\n","Epoch 8 took 0 min and 24 sec for training and 0 min and 1 sec for validation\n","\tTrain Loss: 0.038 | Train Acc: 98.71%\n","\t Val. Loss: 0.521 |  Val. Acc: 86.00%\n","Epoch 9 took 0 min and 25 sec for training and 0 min and 1 sec for validation\n","\tTrain Loss: 0.032 | Train Acc: 98.99%\n","\t Val. Loss: 0.574 |  Val. Acc: 85.61%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"00sUvs8sD_3e","colab_type":"text"},"source":["# Inference on test set"]},{"cell_type":"code","metadata":{"id":"g_VuftmgECHo","colab_type":"code","outputId":"19782fc0-d0df-4266-f5c6-056aca634764","executionInfo":{"status":"ok","timestamp":1590546174559,"user_tz":-480,"elapsed":2338,"user":{"displayName":"Kelvin Heng","photoUrl":"","userId":"16189608668730772404"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["model.load_state_dict(torch.load('model.pt'))\n","test_loss, test_acc, test_time = evaluate_model(model, test_iterator, criterion)\n","print(f'Epoch {epoch} took {convert_time(test_time)} for test')\n","print(f'\\t Test. Loss: {test_loss:.3f} |  Test. Acc: {test_acc*100:.2f}%')"],"execution_count":55,"outputs":[{"output_type":"stream","text":["Epoch 9 took 0 min and 1 sec for test\n","\t Test. Loss: 0.316 |  Test. Acc: 86.88%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4amb0sOlU1RN","colab_type":"text"},"source":["# User Input"]},{"cell_type":"code","metadata":{"id":"WDYbcT7iU20G","colab_type":"code","colab":{}},"source":["import spacy\n","nlp = spacy.load('en')\n","\n","def predict_sentiment(model, sentence, min_len = 5):\n","    # sentence: 'i love this film so much'\n","    model.eval()\n","    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n","    if len(tokenized) < min_len:\n","        tokenized += ['<pad>'] * (min_len - len(tokenized))\n","    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n","    tensor = torch.LongTensor(indexed).to(device)\n","    tensor = tensor.unsqueeze(0)\n","    prediction = torch.sigmoid(model(tensor))\n","    return prediction.item()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MP63IYnYWRjQ","colab_type":"code","outputId":"6b8f8fe0-d50b-4a58-ef59-30194777d7d2","executionInfo":{"status":"ok","timestamp":1590546371137,"user_tz":-480,"elapsed":1498,"user":{"displayName":"Kelvin Heng","photoUrl":"","userId":"16189608668730772404"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["predict_sentiment(model, \"This film is amazing\")"],"execution_count":63,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9917160868644714"]},"metadata":{"tags":[]},"execution_count":63}]},{"cell_type":"code","metadata":{"id":"SPySvg-wWRlm","colab_type":"code","outputId":"b822bbd5-5f46-4211-bdcf-8b6a52068a8a","executionInfo":{"status":"ok","timestamp":1590546388493,"user_tz":-480,"elapsed":814,"user":{"displayName":"Kelvin Heng","photoUrl":"","userId":"16189608668730772404"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["predict_sentiment(model, \"This film is truly terrible\")"],"execution_count":65,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.39685243368148804"]},"metadata":{"tags":[]},"execution_count":65}]}]}