{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"fast_text.ipynb","provenance":[{"file_id":"https://github.com/kelvinheng92/sentiment_analysis/blob/master/baseline.ipynb","timestamp":1589457930206},{"file_id":"https://github.com/kelvinheng92/sentiment_analysis/blob/master/baseline.ipynb","timestamp":1589417551888}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"D0fSKmcvTUU3","colab_type":"text"},"source":["# Data Preparation"]},{"cell_type":"code","metadata":{"id":"ZwrCIld7I2Z_","colab_type":"code","colab":{}},"source":["def generate_bigrams(x):\n","    n_grams = set(zip(*[x[i:] for i in range(2)]))\n","    for n_gram in n_grams:\n","        x.append(' '.join(n_gram))\n","    return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"L1xB2xwRTZKg","colab_type":"code","colab":{}},"source":["import torch\n","from torchtext import data, datasets\n","import random\n","\n","SEED = 1992\n","\n","torch.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True\n","\n","TEXT = data.Field(tokenize = 'spacy', preprocessing = generate_bigrams)\n","LABEL = data.LabelField(dtype = torch.float)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jxyR5l5oTjAh","colab_type":"code","outputId":"7eca1364-5b2b-4f33-9d75-0387ca40828d","executionInfo":{"status":"ok","timestamp":1589538807534,"user_tz":-480,"elapsed":149746,"user":{"displayName":"Kelvin Heng","photoUrl":"","userId":"16189608668730772404"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["downloading aclImdb_v1.tar.gz\n"],"name":"stdout"},{"output_type":"stream","text":["aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:07<00:00, 10.6MB/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"ddKoNplZUY7Q","colab_type":"code","outputId":"c6769f18-af26-451a-e7c7-23fc6c5f198e","executionInfo":{"status":"ok","timestamp":1589538807535,"user_tz":-480,"elapsed":149654,"user":{"displayName":"Kelvin Heng","photoUrl":"","userId":"16189608668730772404"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["print(f'Number of training examples: {len(train_data)}')\n","print(f'Number of testing examples: {len(test_data)}')"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Number of training examples: 25000\n","Number of testing examples: 25000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IhV_vj1GUuZZ","colab_type":"code","outputId":"041c0a8e-1ef3-4462-a4b4-68ab87e4e687","executionInfo":{"status":"ok","timestamp":1589538807536,"user_tz":-480,"elapsed":149568,"user":{"displayName":"Kelvin Heng","photoUrl":"","userId":"16189608668730772404"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["print(vars(train_data.examples[0]))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["{'text': ['Chalk', 'this', 'one', 'up', 'in', 'the', 'win', 'column', ',', 'this', 'was', 'a', 'superb', 'movie', '.', 'The', 'acting', 'performances', 'were', 'great', 'and', 'the', 'script', 'was', 'equally', 'great.<br', '/><br', '/>Helen', 'Hunt', 'was', 'magnificent', 'as', 'the', 'Riverside', 'police', 'officer', 'Gina', 'Pulasky', '.', 'Gina', 'was', 'a', 'complex', 'character', '.', 'She', 'was', 'a', 'rookie', 'cop', 'with', 'the', 'Riverside', 'Police', 'Dept', '.', 'She', 'ended', 'up', 'in', 'an', 'affair', 'with', 'a', 'coworker', 'that', 'she', 'knew', 'had', 'a', 'wife', 'and', 'kids', ',', 'all', 'the', 'while', 'she', 'took', 'on', 'the', 'dangerous', 'task', 'of', 'going', 'undercover', 'to', 'catch', 'a', 'serial', 'killer', '.', '<', 'br', '/><br', '/>Jeff', 'Fahey', '(', 'the', 'Ray', 'Liotta', 'look', 'alike', ')', 'did', 'a', 'bang', 'up', 'job', 'as', 'the', 'confused', ',', 'often', 'stammering', ',', 'police', 'officer', 'that', 'had', 'an', 'affair', 'with', 'Gina', '.', 'He', 'was', 'stoic', 'as', 'an', 'officer', ',', 'but', 'he', 'was', 'quite', 'the', 'opposite', 'when', 'it', 'came', 'to', 'dealing', 'with', 'his', 'feelings', 'and', 'his', 'extra', '-', 'marital', 'affair.<br', '/><br', '/>Steven', 'Weber', ',', 'most', 'notably', 'from', 'the', 'sitcom', '\"', 'Wings', '\"', ',', 'did', 'a', 'nice', 'job', 'as', 'the', 'quiet', ',', 'meek', ',', 'underachieving', 'sociopath', '.', 'On', 'the', 'surface', ',', 'he', 'was', 'an', 'innocent', 'loser', ',', 'but', 'beneath', 'the', 'surface', 'lied', 'a', 'cold', 'hearted', 'killer', '.', 'Weber', 'really', 'took', 'on', 'the', 'persona', 'of', 'a', 'childlike', 'young', 'adult', 'with', 'an', 'insatiable', 'thirst', 'to', 'kill', 'young', 'boys.<br', '/><br', '/>The', 'entire', 'movie', 'was', 'spectacular', '.', 'Each', 'scene', ',', 'each', 'verbal', 'exchange', 'let', 'us', 'know', 'more', 'and', 'more', 'about', 'the', 'characters', '.', 'The', 'production', 'team', 'did', 'a', 'phenomenal', 'job', 'with', 'condensing', 'days', ',', 'if', 'not', 'weeks', 'worth', 'of', 'events', 'into', 'a', '90', 'minute', 'movie', '.', 'This', 'movie', 'was', 'a', 'good', 'example', 'of', 'doing', 'a', 'lot', 'with', 'little', '.', 'a wife', 'win column', 'events into', 'notably from', 'had a', 'character .', 'a good', 'and his', 'script was', 'she knew', '. Weber', 'movie was', 'of events', 'On the', 'Chalk this', 'really took', 'let us', 'going undercover', 'This movie', 'serial killer', '/><br />Helen', 'a lot', ', meek', 'his extra', 'wife and', 'that she', 'in an', 'condensing days', 'team did', 'Riverside Police', 'most notably', 'surface lied', 'was equally', '\" Wings', 'had an', 'spectacular .', 'this one', 'a complex', 'was an', 'bang up', 'movie .', 'She was', 'it came', 'the win', 'acting performances', 'came to', 'officer Gina', 'us know', ', each', '( the', 'exchange let', 'about the', 'Hunt was', '. He', 'Riverside police', 'with a', 'an affair', 'Ray Liotta', 'stoic as', 'the confused', 'when it', '- marital', 'the while', 'officer ,', 'quite the', 'an innocent', 'his feelings', 'each verbal', 'job as', 'was a', 'a coworker', 'look alike', 'an insatiable', 'police officer', 'entire movie', 'scene ,', '. On', 'weeks worth', 'all the', 'confused ,', ', all', '/>The entire', ', if', 'childlike young', 'Gina .', 'a phenomenal', ', often', '< br', 'an officer', 'stammering ,', 'young boys.<br', 'with an', 'did a', 'lot with', 'to kill', 'often stammering', 'good example', 'to catch', 'dealing with', 'he was', 'ended up', 'opposite when', 'from the', 'verbal exchange', '90 minute', 'one up', 'a rookie', ', this', 'a superb', 'meek ,', 'and the', 'job with', 'extra -', 'Weber ,', 'lied a', 'affair with', 'Wings \"', 'days ,', 'alike )', '/><br />Jeff', 'were great', 'equally great.<br', '/>Jeff Fahey', 'insatiable thirst', 'example of', 'production team', 'persona of', 'the quiet', 'loser ,', 'a serial', 'officer that', 'the script', 'as an', 'was magnificent', 'The production', 'She ended', 'of a', 'this was', ', underachieving', 'Each scene', 'thirst to', 'column ,', 'Weber really', 'superb movie', 'young adult', 'quiet ,', 'the characters', '. This', ', did', ', he', 'great and', 'on the', 'to dealing', 'complex character', ', police', 'of going', '. She', 'the Riverside', '. <', 'a bang', 'feelings and', 'sociopath .', 'but beneath', 'dangerous task', 'Fahey (', 'as the', '/>Steven Weber', 'the opposite', 'adult with', 'that had', '/><br />Steven', 'affair.<br /><br', 'kill young', '. The', 'rookie cop', 'Police Dept', ', but', 'Dept .', 'Liotta look', 'know more', 'more about', 'hearted killer', '. Gina', 'of doing', 'was stoic', 'not weeks', 'a 90', 'minute movie', '. Each', ', most', 'marital affair.<br', 'cop with', 'with Gina', 'the persona', '/><br />The', 'but he', 'nice job', 'worth of', 'magnificent as', 'in the', 'underachieving sociopath', 'and kids', '\" ,', 'with the', 'with condensing', '/>Helen Hunt', 'kids ,', 'cold hearted', 'performances were', 'He was', ') did', 'innocent loser', 'boys.<br /><br', 'a cold', 'if not', 'Gina was', 'phenomenal job', 'great.<br /><br', 'with his', 'while she', 'with little', 'characters .', 'beneath the', 'and more', 'little .', 'took on', 'up in', 'catch a', 'was spectacular', 'knew had', 'task of', 'The acting', 'Gina Pulasky', 'Pulasky .', 'sitcom \"', 'the Ray', 'surface ,', 'was quite', 'more and', 'she took', 'undercover to', 'killer .', 'a nice', 'coworker that', 'the sitcom', 'a childlike', 'doing a', 'into a', 'br /><br', 'the surface', 'up job', 'the dangerous'], 'label': 'pos'}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"byg-goSAjMy1","colab_type":"code","colab":{}},"source":["valid_data, test_data = test_data.split(split_ratio = 0.5, random_state = random.seed(SEED)) #Further split test set into validation and test set"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OT_sjF_ZlMCg","colab_type":"code","outputId":"36cecb57-b447-475b-9408-ad7b61a6936f","executionInfo":{"status":"ok","timestamp":1589538807537,"user_tz":-480,"elapsed":149411,"user":{"displayName":"Kelvin Heng","photoUrl":"","userId":"16189608668730772404"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["print(f'Number of training examples: {len(train_data)}')\n","print(f'Number of validation examples: {len(valid_data)}')\n","print(f'Number of testing examples: {len(test_data)}')"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Number of training examples: 25000\n","Number of validation examples: 12500\n","Number of testing examples: 12500\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DW8OoSamoN_5","colab_type":"code","outputId":"1e3c349b-ee5f-48bd-ea4c-b3462b931115","executionInfo":{"status":"ok","timestamp":1589539259867,"user_tz":-480,"elapsed":601656,"user":{"displayName":"Kelvin Heng","photoUrl":"","userId":"16189608668730772404"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["MAX_VOCAB_SIZE = 25_000\n","TEXT.build_vocab(train_data,\n","                 max_size = MAX_VOCAB_SIZE,\n","                 vectors = \"glove.6B.100d\",\n","                 unk_init = torch.Tensor.normal_)\n","LABEL.build_vocab(train_data)"],"execution_count":9,"outputs":[{"output_type":"stream","text":[".vector_cache/glove.6B.zip: 862MB [06:30, 2.21MB/s]\n","100%|█████████▉| 399338/400000 [00:22<00:00, 16411.89it/s]"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"xKpI5orppdF4","colab_type":"code","outputId":"83338ef1-5115-412f-846f-19d99dc326bb","executionInfo":{"status":"ok","timestamp":1589539259868,"user_tz":-480,"elapsed":601567,"user":{"displayName":"Kelvin Heng","photoUrl":"","userId":"16189608668730772404"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["vars(LABEL.vocab)"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'freqs': Counter({'neg': 12500, 'pos': 12500}),\n"," 'itos': ['neg', 'pos'],\n"," 'stoi': defaultdict(<function torchtext.vocab._default_unk_index>,\n","             {'neg': 0, 'pos': 1}),\n"," 'vectors': None}"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"_zHC1He0oqbn","colab_type":"code","outputId":"d41f85e3-ea56-4f58-ad2d-8183a9b14e78","executionInfo":{"status":"ok","timestamp":1589539259869,"user_tz":-480,"elapsed":601479,"user":{"displayName":"Kelvin Heng","photoUrl":"","userId":"16189608668730772404"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["print(f'There are {len(TEXT.vocab)} unique tokens in TEXT vocabulary')\n","print(f'There are {len(LABEL.vocab)} unique tokens in LABEL vocabulary')"],"execution_count":11,"outputs":[{"output_type":"stream","text":["There are 25002 unique tokens in TEXT vocabulary\n","There are 2 unique tokens in LABEL vocabulary\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mwaiWFisr1IW","colab_type":"code","outputId":"a4aeea86-a604-4890-d01a-ee24592f417a","executionInfo":{"status":"ok","timestamp":1589539259869,"user_tz":-480,"elapsed":601397,"user":{"displayName":"Kelvin Heng","photoUrl":"","userId":"16189608668730772404"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["print(vars(TEXT.vocab).keys())\n","print(vars(LABEL.vocab).keys())"],"execution_count":12,"outputs":[{"output_type":"stream","text":["dict_keys(['freqs', 'itos', 'stoi', 'vectors'])\n","dict_keys(['freqs', 'itos', 'stoi', 'vectors'])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Oa95Dc4WrQ4l","colab_type":"code","colab":{}},"source":["BATCH_SIZE = 64\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits((train_data, valid_data, test_data),\n","                                                                           batch_size = BATCH_SIZE,\n","                                                                           device = device)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zk5IUombrJRC","colab_type":"text"},"source":["# Model Architecture"]},{"cell_type":"code","metadata":{"id":"V0GhJG2Jt3I_","colab_type":"code","colab":{}},"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class fast_text(nn.Module):\n","  def __init__(self, input_dim, emb_dim, output_dim, dropout, pad_idx):\n","    \n","    super().__init__() # init from super class in nn.module\n","    self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx = pad_idx)\n","    self.fc = nn.Linear(emb_dim, output_dim)\n","    self.dropout = nn.Dropout(dropout)\n","\n","  def forward(self, text):\n","    # text has dimension of (sentence_length, batch_size)\n","    embedding_output = self.dropout(self.embedding(text))\n","    # embedding_output has dimension of (sentence_length, batch_size, emb_dim)\n","    embedding_output = embedding_output.permute(1, 0, 2)\n","    # embedding_output has dimension of (batch_size, sentence_length, emb_dim)\n","    pooled = F.avg_pool2d(embedding_output, (embedding_output.shape[1], 1))\n","    # pooled has dimension of (batch_size, 1, emb_dim)\n","    pooled = pooled.squeeze()\n","    # pooled has dimension of (batch_size, emb_dim)\n","    fc_output = self.fc(pooled)\n","    # fc_output has dimension of (batch_size, output_dim) after fc layer\n","\n","    return fc_output"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9JWdGeNhrE7M","colab_type":"code","colab":{}},"source":["INPUT_DIM = len(TEXT.vocab)\n","EMB_DIM = 100\n","OUTPUT_DIM = 1\n","DROPOUT = 0.5\n","PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n","\n","model = fast_text(INPUT_DIM, EMB_DIM, OUTPUT_DIM, DROPOUT, PAD_IDX)\n","model = model.to(device)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yZPN1y6es4zE","colab_type":"code","outputId":"8e49b2e0-ccb6-4ba7-84f7-869025f0287b","executionInfo":{"status":"ok","timestamp":1589539678859,"user_tz":-480,"elapsed":1427,"user":{"displayName":"Kelvin Heng","photoUrl":"","userId":"16189608668730772404"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'The model has {count_parameters(model):,} trainable parameters')"],"execution_count":26,"outputs":[{"output_type":"stream","text":["The model has 2,500,301 trainable parameters\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UqsidqA3OYG2","colab_type":"code","outputId":"d87ed24f-3e60-41da-fb5e-c2167cd32cab","executionInfo":{"status":"ok","timestamp":1589539679484,"user_tz":-480,"elapsed":2006,"user":{"displayName":"Kelvin Heng","photoUrl":"","userId":"16189608668730772404"}},"colab":{"base_uri":"https://localhost:8080/","height":170}},"source":["pretrained_embeddings = TEXT.vocab.vectors\n","print(pretrained_embeddings.shape)\n","model.embedding.weight.data.copy_(pretrained_embeddings)"],"execution_count":27,"outputs":[{"output_type":"stream","text":["torch.Size([25002, 100])\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.3358, -0.0554,  0.3294,  ...,  0.8827,  0.0913, -2.2403],\n","        [-0.0548, -0.7422,  0.8423,  ..., -0.6081,  1.1196, -1.0543],\n","        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n","        ...,\n","        [-0.3432, -0.4694,  1.3178,  ..., -0.9612,  0.5312,  0.8897],\n","        [-0.0935, -1.1532,  0.1853,  ...,  0.8103, -0.8533, -1.4943],\n","        [-1.5623, -1.3385,  0.2712,  ...,  0.4855, -1.3892, -1.7497]],\n","       device='cuda:0')"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"id":"9WoROUbwO4zJ","colab_type":"code","outputId":"3f49334d-7334-43e6-d362-1ed14475c5dd","executionInfo":{"status":"ok","timestamp":1589539679484,"user_tz":-480,"elapsed":1888,"user":{"displayName":"Kelvin Heng","photoUrl":"","userId":"16189608668730772404"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n","model.embedding.weight.data[UNK_IDX] = torch.zeros(EMB_DIM)\n","model.embedding.weight.data[PAD_IDX] = torch.zeros(EMB_DIM)\n","print(model.embedding.weight.data)"],"execution_count":28,"outputs":[{"output_type":"stream","text":["tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n","        ...,\n","        [-0.3432, -0.4694,  1.3178,  ..., -0.9612,  0.5312,  0.8897],\n","        [-0.0935, -1.1532,  0.1853,  ...,  0.8103, -0.8533, -1.4943],\n","        [-1.5623, -1.3385,  0.2712,  ...,  0.4855, -1.3892, -1.7497]],\n","       device='cuda:0')\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tvB3PqOJtwzj","colab_type":"text"},"source":["# Define loss, metric and optimizer"]},{"cell_type":"code","metadata":{"id":"Q9AbMu09uOGk","colab_type":"code","colab":{}},"source":["import torch.optim as optim\n","\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","criterion = nn.BCEWithLogitsLoss()\n","criterion = criterion.to(device)\n","\n","def binary_accuracy(preds, label):\n","\n","    rounded_preds = torch.round(torch.sigmoid(preds))\n","    correct = (rounded_preds == label).float() \n","    acc = correct.sum() / len(correct)\n","    return acc"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3fvPvuJKv1J0","colab_type":"text"},"source":["# Train Model"]},{"cell_type":"code","metadata":{"id":"0Z4ax_BIv4n0","colab_type":"code","colab":{}},"source":["import time\n","\n","def train_model(model, iterator, optimizer, criterion):\n","  # start of new epoch \n","  start_time = time.time()\n","  epoch_loss = 0\n","  epoch_acc = 0\n","  model.train() # set model to train mdoe to enable dropout and batch norm\n","  batch_size = len(iterator)\n","\n","  for batch in iterator:\n","    optimizer.zero_grad() # set optimizer grad to 0 first\n","    pred = model(batch.text).squeeze(1) # forward prop\n","    loss = criterion(pred, batch.label) # calculate loss \n","    acc = binary_accuracy(pred, batch.label) # calculate metric\n","    loss.backward() # backward prop\n","    optimizer.step() # updates optimizer parameters\n","    epoch_loss += loss.item()\n","    epoch_acc += acc.item()\n","\n","  end_time = time.time()\n","  epoch_time_taken = end_time - start_time\n","\n","  return epoch_loss / batch_size, epoch_acc / batch_size, epoch_time_taken"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ROn2vg4hykAw","colab_type":"code","colab":{}},"source":["from datetime import datetime\n","\n","def evaluate_model(model, iterator, criterion):\n","  # start of new epoch \n","  start_time = time.time()\n","  epoch_loss = 0\n","  epoch_acc = 0\n","  batch_size = len(iterator)\n","  model.eval() # set model to eval mode\n","\n","  for batch in iterator:\n","    pred = model(batch.text).squeeze(1) # forward prop\n","    loss = criterion(pred, batch.label) # calculate loss \n","    acc = binary_accuracy(pred, batch.label) # calculate metric\n","    epoch_loss += loss.item()\n","    epoch_acc += acc.item()\n","\n","  end_time = time.time()\n","  epoch_time_taken = end_time - start_time\n","\n","  return epoch_loss / batch_size, epoch_acc / batch_size, epoch_time_taken"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HqmKMQMp1mV4","colab_type":"code","colab":{}},"source":["import time\n","\n","def convert_time(time_taken):\n","    elapsed_mins = int(time_taken / 60)\n","    elapsed_secs = int(time_taken - (elapsed_mins * 60))\n","    output = f'{elapsed_mins} min and {elapsed_secs} sec'\n","    return output"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZwsXbles0lUZ","colab_type":"code","outputId":"fe713c12-9284-46f6-b63e-a1b3c72dfaf9","executionInfo":{"status":"ok","timestamp":1589539863062,"user_tz":-480,"elapsed":184593,"user":{"displayName":"Kelvin Heng","photoUrl":"","userId":"16189608668730772404"}},"colab":{"base_uri":"https://localhost:8080/","height":527}},"source":["MAX_EPOCHS = 10\n","\n","best_loss = float('inf')\n","for epoch in range(MAX_EPOCHS):\n","  train_loss, train_acc, train_time = train_model(model, train_iterator, optimizer, criterion)\n","  valid_loss, valid_acc, valid_time = evaluate_model(model, valid_iterator, criterion)\n","  print(f'Epoch {epoch} took {convert_time(train_time)} for training and {convert_time(valid_time)} for validation')\n","\n","  if valid_loss < best_loss:\n","    best_loss = valid_loss\n","    torch.save(model.state_dict(), 'model.pt')\n","\n","  print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","  print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n"],"execution_count":33,"outputs":[{"output_type":"stream","text":["Epoch 0 took 0 min and 15 sec for training and 0 min and 2 sec for validation\n","\tTrain Loss: 0.685 | Train Acc: 58.61%\n","\t Val. Loss: 0.610 |  Val. Acc: 69.69%\n","Epoch 1 took 0 min and 15 sec for training and 0 min and 2 sec for validation\n","\tTrain Loss: 0.627 | Train Acc: 74.87%\n","\t Val. Loss: 0.464 |  Val. Acc: 77.98%\n","Epoch 2 took 0 min and 15 sec for training and 0 min and 2 sec for validation\n","\tTrain Loss: 0.535 | Train Acc: 81.68%\n","\t Val. Loss: 0.392 |  Val. Acc: 82.99%\n","Epoch 3 took 0 min and 15 sec for training and 0 min and 2 sec for validation\n","\tTrain Loss: 0.451 | Train Acc: 85.55%\n","\t Val. Loss: 0.381 |  Val. Acc: 85.39%\n","Epoch 4 took 0 min and 15 sec for training and 0 min and 2 sec for validation\n","\tTrain Loss: 0.391 | Train Acc: 87.53%\n","\t Val. Loss: 0.390 |  Val. Acc: 86.65%\n","Epoch 5 took 0 min and 15 sec for training and 0 min and 2 sec for validation\n","\tTrain Loss: 0.349 | Train Acc: 88.89%\n","\t Val. Loss: 0.407 |  Val. Acc: 87.30%\n","Epoch 6 took 0 min and 15 sec for training and 0 min and 2 sec for validation\n","\tTrain Loss: 0.316 | Train Acc: 89.70%\n","\t Val. Loss: 0.425 |  Val. Acc: 87.91%\n","Epoch 7 took 0 min and 15 sec for training and 0 min and 2 sec for validation\n","\tTrain Loss: 0.290 | Train Acc: 90.52%\n","\t Val. Loss: 0.442 |  Val. Acc: 88.46%\n","Epoch 8 took 0 min and 15 sec for training and 0 min and 2 sec for validation\n","\tTrain Loss: 0.269 | Train Acc: 91.16%\n","\t Val. Loss: 0.455 |  Val. Acc: 88.89%\n","Epoch 9 took 0 min and 15 sec for training and 0 min and 2 sec for validation\n","\tTrain Loss: 0.252 | Train Acc: 91.60%\n","\t Val. Loss: 0.470 |  Val. Acc: 89.29%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"00sUvs8sD_3e","colab_type":"text"},"source":["# Inference on test set"]},{"cell_type":"code","metadata":{"id":"g_VuftmgECHo","colab_type":"code","outputId":"526013ed-211d-4c96-fed8-ee1cf7482976","executionInfo":{"status":"ok","timestamp":1589540250562,"user_tz":-480,"elapsed":4940,"user":{"displayName":"Kelvin Heng","photoUrl":"","userId":"16189608668730772404"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["model.load_state_dict(torch.load('model.pt'))\n","test_loss, test_acc, test_time = evaluate_model(model, test_iterator, criterion)\n","print(f'Epoch {epoch} took {convert_time(test_time)} for test')\n","print(f'\\t Test. Loss: {test_loss:.3f} |  Test. Acc: {test_acc*100:.2f}%')"],"execution_count":35,"outputs":[{"output_type":"stream","text":["Epoch 9 took 0 min and 2 sec for test\n","\t Test. Loss: 0.392 |  Test. Acc: 84.92%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4amb0sOlU1RN","colab_type":"text"},"source":["# User Input"]},{"cell_type":"code","metadata":{"id":"WDYbcT7iU20G","colab_type":"code","colab":{}},"source":["import spacy\n","nlp = spacy.load('en')\n","\n","def predict_sentiment(model, sentence):\n","    # sentence: 'i love this film so much'\n","    model.eval()\n","    tokenized = generate_bigrams([tok.text for tok in nlp.tokenizer(sentence)])\n","    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n","    tensor = torch.LongTensor(indexed).to(device)\n","    tensor = tensor.unsqueeze(1)\n","    prediction = torch.sigmoid(model(tensor))\n","    return prediction.item()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MP63IYnYWRjQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"2559c6a4-04b2-4a93-ea2f-fbafeb8c7feb","executionInfo":{"status":"ok","timestamp":1589540251777,"user_tz":-480,"elapsed":6138,"user":{"displayName":"Kelvin Heng","photoUrl":"","userId":"16189608668730772404"}}},"source":["predict_sentiment(model, \"This film is amazing\")"],"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1.0"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"code","metadata":{"id":"SPySvg-wWRlm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"a75525bf-cfbf-4cd1-82e4-faa0138e03b7","executionInfo":{"status":"ok","timestamp":1589540251777,"user_tz":-480,"elapsed":6020,"user":{"displayName":"Kelvin Heng","photoUrl":"","userId":"16189608668730772404"}}},"source":["predict_sentiment(model, \"This film is not awful\")"],"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3.6655790609074757e-05"]},"metadata":{"tags":[]},"execution_count":38}]}]}