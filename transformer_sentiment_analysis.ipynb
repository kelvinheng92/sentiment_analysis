{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"transformer_sentiment_analysis.ipynb","provenance":[{"file_id":"https://github.com/kelvinheng92/sentiment_analysis/blob/master/baseline.ipynb","timestamp":1589457930206},{"file_id":"https://github.com/kelvinheng92/sentiment_analysis/blob/master/baseline.ipynb","timestamp":1589417551888}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"D0fSKmcvTUU3","colab_type":"text"},"source":["# Data Preparation"]},{"cell_type":"code","metadata":{"id":"L1xB2xwRTZKg","colab_type":"code","colab":{}},"source":["import torch\n","from torchtext import data, datasets\n","import random\n","import numpy as np\n","\n","SEED = 1992\n","\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QslNbEaMhd3q","colab_type":"code","colab":{}},"source":["# !pip install transformers\n","from transformers import BertTokenizer\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"juybnjO_iMcr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"7a0f64bf-af13-45aa-cb4a-d324ff18bc3f","executionInfo":{"status":"ok","timestamp":1590812232921,"user_tz":-480,"elapsed":5015,"user":{"displayName":"Kelvin Heng","photoUrl":"","userId":"16189608668730772404"}}},"source":["len(tokenizer.vocab)"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["30522"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"sPNS-aN_iSTD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"75526a5a-f15f-49c5-e06a-886ede7baec8","executionInfo":{"status":"ok","timestamp":1590812232922,"user_tz":-480,"elapsed":5007,"user":{"displayName":"Kelvin Heng","photoUrl":"","userId":"16189608668730772404"}}},"source":["tokens = tokenizer.tokenize('Hello WORLD how ARE yoU?') \n","print(tokens) # it is bert base uncased\n","\n","indexes = tokenizer.convert_tokens_to_ids(tokens)\n","print(indexes)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["['hello', 'world', 'how', 'are', 'you', '?']\n","[7592, 2088, 2129, 2024, 2017, 1029]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7IHmNZjhinu2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"f0cd5166-e1f0-4de2-ef7e-73b1db27ea61","executionInfo":{"status":"ok","timestamp":1590812232923,"user_tz":-480,"elapsed":5000,"user":{"displayName":"Kelvin Heng","photoUrl":"","userId":"16189608668730772404"}}},"source":["init_token = tokenizer.cls_token\n","eos_token = tokenizer.sep_token\n","pad_token = tokenizer.pad_token\n","unk_token = tokenizer.unk_token\n","\n","print(init_token, eos_token, pad_token, unk_token)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["[CLS] [SEP] [PAD] [UNK]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DCbunRAVjA5I","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"19a88ee3-6406-4545-dbb0-e23c569ea496","executionInfo":{"status":"ok","timestamp":1590812232924,"user_tz":-480,"elapsed":4992,"user":{"displayName":"Kelvin Heng","photoUrl":"","userId":"16189608668730772404"}}},"source":["max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n","\n","print(max_input_length)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["512\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Z1Mt9ECMjKqy","colab_type":"code","colab":{}},"source":["def tokenize_and_cut(sentence):\n","    tokens = tokenizer.tokenize(sentence) \n","    tokens = tokens[:max_input_length-2]\n","    return tokens"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DgjoZ0j_jRVa","colab_type":"code","colab":{}},"source":["TEXT = data.Field(batch_first = True,\n","                  use_vocab = False,\n","                  tokenize = tokenize_and_cut,\n","                  preprocessing = tokenizer.convert_tokens_to_ids,\n","                  init_token = tokenizer.cls_token_id,\n","                  eos_token = tokenizer.sep_token_id,\n","                  pad_token = tokenizer.pad_token_id,\n","                  unk_token = tokenizer.unk_token_id)\n","LABEL = data.LabelField(dtype = torch.float)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jxyR5l5oTjAh","colab_type":"code","colab":{}},"source":["train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n","valid_data, test_data = test_data.split(split_ratio = 0.5, random_state = random.seed(SEED)) #Further split test set into validation and test set"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ddKoNplZUY7Q","colab_type":"code","outputId":"562f3d62-a3a5-49fe-8415-c5baab30b153","executionInfo":{"status":"ok","timestamp":1590812444672,"user_tz":-480,"elapsed":216710,"user":{"displayName":"Kelvin Heng","photoUrl":"","userId":"16189608668730772404"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["print(f'Number of training examples: {len(train_data)}')\n","print(f'Number of testing examples: {len(test_data)}')"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Number of training examples: 25000\n","Number of testing examples: 12500\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IhV_vj1GUuZZ","colab_type":"code","outputId":"a5a6447c-c950-42ab-c1e5-6bd948299e4f","executionInfo":{"status":"ok","timestamp":1590812444672,"user_tz":-480,"elapsed":216700,"user":{"displayName":"Kelvin Heng","photoUrl":"","userId":"16189608668730772404"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["print(vars(train_data.examples[0]))"],"execution_count":11,"outputs":[{"output_type":"stream","text":["{'text': [2006, 4678, 6642, 1037, 2143, 3310, 2247, 2008, 2038, 1996, 2373, 2000, 7818, 1996, 2568, 1010, 4010, 1996, 2540, 1998, 3543, 1996, 2200, 3969, 1012, 1000, 10223, 1000, 2003, 2107, 1037, 2143, 1012, 1045, 2288, 1000, 10223, 1000, 2013, 2026, 2564, 2040, 2288, 2009, 2013, 1037, 11429, 2040, 2003, 1999, 1996, 2143, 2449, 1012, 2016, 3427, 2009, 2005, 1037, 2117, 2051, 2007, 2033, 1012, 2057, 2020, 2119, 4372, 2705, 7941, 3709, 1012, 2014, 2004, 2065, 2005, 1996, 2034, 2051, 2153, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1000, 10223, 1000, 2003, 1037, 8687, 3538, 2881, 2000, 4604, 2017, 2067, 2000, 1996, 2617, 2012, 2029, 2035, 1997, 2115, 16547, 2318, 2635, 2173, 1012, 2009, 2515, 2023, 2096, 2108, 21660, 2135, 14036, 1012, 25626, 12385, 1005, 1055, 3772, 1998, 15732, 2004, 1037, 2472, 2079, 2025, 2292, 2017, 2298, 2185, 2013, 1996, 3898, 1012, 2002, 14030, 1037, 2839, 2029, 4487, 10286, 5244, 2007, 1037, 12883, 16291, 2066, 1010, 24646, 19567, 12660, 1010, 2021, 22775, 3344, 2007, 2107, 2104, 13068, 2098, 23997, 2008, 2017, 5293, 2017, 1005, 2128, 3666, 1037, 3185, 1012, 2043, 1996, 4958, 11514, 4819, 2100, 4978, 2076, 1996, 8235, 14463, 1010, 1045, 2387, 2026, 2564, 1999, 4000, 2005, 1996, 2117, 2051, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2004, 1037, 2166, 2873, 1010, 1045, 10956, 3265, 3930, 1998, 8651, 1010, 1998, 2023, 2143, 2003, 1037, 1000, 2442, 2156, 1000, 2005, 2166, 7850, 1998, 3087, 6224, 2037, 2219, 3167, 3930, 1998, 8651, 1012, 2009, 2003, 1037, 8235, 1010, 5541, 17743, 2007, 1996, 2373, 2000, 2689, 3268, 999], 'label': 'pos'}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ERIRIHo2lSfu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"cb00d36b-49ab-4cf1-b246-5edc525b8e39","executionInfo":{"status":"ok","timestamp":1590812444673,"user_tz":-480,"elapsed":216692,"user":{"displayName":"Kelvin Heng","photoUrl":"","userId":"16189608668730772404"}}},"source":["print(tokenizer.convert_ids_to_tokens(vars(train_data.examples[0])['text']))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["['on', 'rare', 'occasions', 'a', 'film', 'comes', 'along', 'that', 'has', 'the', 'power', 'to', 'expand', 'the', 'mind', ',', 'warm', 'the', 'heart', 'and', 'touch', 'the', 'very', 'soul', '.', '\"', 'lou', '\"', 'is', 'such', 'a', 'film', '.', 'i', 'got', '\"', 'lou', '\"', 'from', 'my', 'wife', 'who', 'got', 'it', 'from', 'a', 'neighbor', 'who', 'is', 'in', 'the', 'film', 'business', '.', 'she', 'watched', 'it', 'for', 'a', 'second', 'time', 'with', 'me', '.', 'we', 'were', 'both', 'en', '##th', '##ral', '##led', '.', 'her', 'as', 'if', 'for', 'the', 'first', 'time', 'again', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', '\"', 'lou', '\"', 'is', 'a', 'magical', 'piece', 'designed', 'to', 'send', 'you', 'back', 'to', 'the', 'moment', 'at', 'which', 'all', 'of', 'your', 'dramas', 'started', 'taking', 'place', '.', 'it', 'does', 'this', 'while', 'being', 'relentless', '##ly', 'entertaining', '.', 'bret', 'carr', \"'\", 's', 'acting', 'and', 'pacing', 'as', 'a', 'director', 'do', 'not', 'let', 'you', 'look', 'away', 'from', 'the', 'screen', '.', 'he', 'crafts', 'a', 'character', 'which', 'di', '##sar', '##ms', 'with', 'a', 'bugs', 'bunny', 'like', ',', 'stu', '##ttering', 'innocence', ',', 'but', 'warmly', 'carried', 'with', 'such', 'under', '##play', '##ed', 'sincerity', 'that', 'you', 'forget', 'you', \"'\", 're', 'watching', 'a', 'movie', '.', 'when', 'the', 'ep', '##ip', '##han', '##y', 'hits', 'during', 'the', 'brilliant', 'climax', ',', 'i', 'saw', 'my', 'wife', 'in', 'tears', 'for', 'the', 'second', 'time', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'as', 'a', 'life', 'coach', ',', 'i', 'facilitate', 'individual', 'growth', 'and', 'transformation', ',', 'and', 'this', 'film', 'is', 'a', '\"', 'must', 'see', '\"', 'for', 'life', 'coaches', 'and', 'anyone', 'seeking', 'their', 'own', 'personal', 'growth', 'and', 'transformation', '.', 'it', 'is', 'a', 'brilliant', ',', 'creative', 'masterpiece', 'with', 'the', 'power', 'to', 'change', 'lives', '!']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OT_sjF_ZlMCg","colab_type":"code","outputId":"a0aa8578-4816-4525-ccc2-408da71c3c1c","executionInfo":{"status":"ok","timestamp":1590812444673,"user_tz":-480,"elapsed":216682,"user":{"displayName":"Kelvin Heng","photoUrl":"","userId":"16189608668730772404"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["print(f'Number of training examples: {len(train_data)}')\n","print(f'Number of validation examples: {len(valid_data)}')\n","print(f'Number of testing examples: {len(test_data)}')"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Number of training examples: 25000\n","Number of validation examples: 12500\n","Number of testing examples: 12500\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BUK4kMg4lqNw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":102},"outputId":"b9f6ae6a-ca60-4d2f-9e9c-2efc6b002e95","executionInfo":{"status":"ok","timestamp":1590812444674,"user_tz":-480,"elapsed":216674,"user":{"displayName":"Kelvin Heng","photoUrl":"","userId":"16189608668730772404"}}},"source":["LABEL.build_vocab(train_data)\n","vars(LABEL.vocab)"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'freqs': Counter({'neg': 12500, 'pos': 12500}),\n"," 'itos': ['neg', 'pos'],\n"," 'stoi': defaultdict(<function torchtext.vocab._default_unk_index>,\n","             {'neg': 0, 'pos': 1}),\n"," 'vectors': None}"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"mwaiWFisr1IW","colab_type":"code","outputId":"af977d1c-99fd-4835-aeec-389900074c0d","executionInfo":{"status":"ok","timestamp":1590812444675,"user_tz":-480,"elapsed":216666,"user":{"displayName":"Kelvin Heng","photoUrl":"","userId":"16189608668730772404"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(vars(LABEL.vocab).keys())"],"execution_count":15,"outputs":[{"output_type":"stream","text":["dict_keys(['freqs', 'itos', 'stoi', 'vectors'])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Oa95Dc4WrQ4l","colab_type":"code","colab":{}},"source":["BATCH_SIZE = 128\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits((train_data, valid_data, test_data),\n","                                                                           batch_size = BATCH_SIZE, device = device)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_DdUrlNIpnEB","colab_type":"code","colab":{}},"source":["from transformers import BertModel\n","\n","bert = BertModel.from_pretrained('bert-base-uncased')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zk5IUombrJRC","colab_type":"text"},"source":["# Model Architecture"]},{"cell_type":"code","metadata":{"id":"V0GhJG2Jt3I_","colab_type":"code","colab":{}},"source":["import torch.nn as nn\n","\n","class BERTGRU(nn.Module):\n","  def __init__(self, bert, hidden_dim, output_dim,\n","               num_layers, bidirectional, dropout):\n","    \n","    super().__init__() # init from super class in nn.module\n","\n","    self.bert = bert\n","    emb_dim = bert.config.to_dict()['hidden_size']\n","    self.rnn = nn.GRU(emb_dim, hidden_dim, num_layers = num_layers,\n","                      bidirectional = bidirectional, batch_first = True,\n","                      dropout = 0 if num_layers < 2 else dropout)\n","    \n","    num_direction = 2 if bidirectional else 1\n","    self.fc = nn.Linear(hidden_dim * num_direction, output_dim)\n","    self.dropout = nn.Dropout(dropout)\n","\n","  def forward(self, text):\n","    # text has dimension of (batch_size, sentence_length)\n","    with torch.no_grad():\n","      embedding_output = self.bert(text)[0]\n","      # embedding_output has dimension of (batch_size, sentence_length, emb_dim)\n","\n","    _, hidden_output = self.rnn(embedding_output)\n","    # hidden_output has dimension of (num_directions * num_layers, batch_size, hidden_dim) where num_directions = 2 if bidirectional and num_layers (if it is stacked RNN)\n","\n","    if self.rnn.bidirectional:\n","      hidden_output = self.dropout(torch.cat((hidden_output[-2,:,:], hidden_output[-1,:,:]), dim = 1))\n","    else:\n","      hidden_output = self.dropout(hidden_output[-1,:,:])\n","    # hidden_output = [batch size, hidden_dim]\n","\n","    fc_output = self.fc(hidden_output)\n","    # fc_output has dimension of (batch_size, output_dim) after fc layer\n","\n","    return fc_output"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9JWdGeNhrE7M","colab_type":"code","colab":{}},"source":["HIDDEN_DIM = 256\n","OUTPUT_DIM = 1\n","NUM_LAYERS = 2\n","BIDIRECTIONAL = True\n","DROPOUT = 0.25\n","\n","model = BERTGRU(bert, HIDDEN_DIM, OUTPUT_DIM,\n","            NUM_LAYERS, BIDIRECTIONAL, DROPOUT)\n","model = model.to(device)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yZPN1y6es4zE","colab_type":"code","outputId":"04ee6599-8117-457d-8d06-0e881f643aff","executionInfo":{"status":"ok","timestamp":1590812452105,"user_tz":-480,"elapsed":224058,"user":{"displayName":"Kelvin Heng","photoUrl":"","userId":"16189608668730772404"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'The model has {count_parameters(model):,} trainable parameters')"],"execution_count":20,"outputs":[{"output_type":"stream","text":["The model has 112,241,409 trainable parameters\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Djo0caUlx_ns","colab_type":"code","colab":{}},"source":["for name, param in model.named_parameters():                \n","    if name.startswith('bert'):\n","        param.requires_grad = False"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tvB3PqOJtwzj","colab_type":"text"},"source":["# Define loss, metric and optimizer"]},{"cell_type":"code","metadata":{"id":"Q9AbMu09uOGk","colab_type":"code","colab":{}},"source":["import torch.optim as optim\n","\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","criterion = nn.BCEWithLogitsLoss()\n","criterion = criterion.to(device)\n","\n","def binary_accuracy(preds, label):\n","\n","    rounded_preds = torch.round(torch.sigmoid(preds))\n","    correct = (rounded_preds == label).float() \n","    acc = correct.sum() / len(correct)\n","    return acc"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3fvPvuJKv1J0","colab_type":"text"},"source":["# Train Model"]},{"cell_type":"code","metadata":{"id":"0Z4ax_BIv4n0","colab_type":"code","colab":{}},"source":["import time\n","\n","def train_model(model, iterator, optimizer, criterion):\n","  # start of new epoch \n","  start_time = time.time()\n","  epoch_loss = 0\n","  epoch_acc = 0\n","  model.train() # set model to train mdoe to enable dropout and batch norm\n","  batch_size = len(iterator)\n","\n","  for batch in iterator:\n","    optimizer.zero_grad() # set optimizer grad to 0 first\n","    pred = model(batch.text).squeeze(1) # forward prop\n","    loss = criterion(pred, batch.label) # calculate loss \n","    acc = binary_accuracy(pred, batch.label) # calculate metric\n","    loss.backward() # backward prop\n","    optimizer.step() # updates optimizer parameters\n","    epoch_loss += loss.item()\n","    epoch_acc += acc.item()\n","\n","  end_time = time.time()\n","  epoch_time_taken = end_time - start_time\n","\n","  return epoch_loss / batch_size, epoch_acc / batch_size, epoch_time_taken"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ROn2vg4hykAw","colab_type":"code","colab":{}},"source":["from datetime import datetime\n","\n","def evaluate_model(model, iterator, criterion):\n","  # start of new epoch \n","  start_time = time.time()\n","  epoch_loss = 0\n","  epoch_acc = 0\n","  batch_size = len(iterator)\n","  model.eval() # set model to eval mode\n","\n","  for batch in iterator:\n","\n","    pred = model(batch.text).squeeze(1) # forward prop\n","    loss = criterion(pred, batch.label) # calculate loss \n","    acc = binary_accuracy(pred, batch.label) # calculate metric\n","    epoch_loss += loss.item()\n","    epoch_acc += acc.item()\n","\n","  end_time = time.time()\n","  epoch_time_taken = end_time - start_time\n","\n","  return epoch_loss / batch_size, epoch_acc / batch_size, epoch_time_taken"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HqmKMQMp1mV4","colab_type":"code","colab":{}},"source":["import time\n","\n","def convert_time(time_taken):\n","    elapsed_mins = int(time_taken / 60)\n","    elapsed_secs = int(time_taken - (elapsed_mins * 60))\n","    output = f'{elapsed_mins} min and {elapsed_secs} sec'\n","    return output"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZwsXbles0lUZ","colab_type":"code","outputId":"1e5e2518-b8c7-4aba-ff19-cca98f7f4339","executionInfo":{"status":"ok","timestamp":1589508741883,"user_tz":-480,"elapsed":1127519,"user":{"displayName":"Kelvin Heng","photoUrl":"","userId":"16189608668730772404"}},"colab":{"base_uri":"https://localhost:8080/","height":170}},"source":["MAX_EPOCHS = 10\n","\n","best_loss = float('inf')\n","for epoch in range(MAX_EPOCHS):\n","  train_loss, train_acc, train_time = train_model(model, train_iterator, optimizer, criterion)\n","  valid_loss, valid_acc, valid_time = evaluate_model(model, valid_iterator, criterion)\n","  print(f'Epoch {epoch} took {convert_time(train_time)} for training and {convert_time(valid_time)} for validation')\n","\n","  if valid_loss < best_loss:\n","    best_loss = valid_loss\n","    torch.save(model.state_dict(), 'model.pt')\n","\n","  print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","  print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 0 took 20 min and 2 sec for training and 4 min and 14 sec for validation\n","\tTrain Loss: 0.437 | Train Acc: 78.47%\n","\t Val. Loss: 0.246 |  Val. Acc: 89.88%\n","Epoch 1 took 20 min and 8 sec for training and 4 min and 15 sec for validation\n","\tTrain Loss: 0.258 | Train Acc: 89.68%\n","\t Val. Loss: 0.225 |  Val. Acc: 91.06%\n","Epoch 2 took 20 min and 11 sec for training and 4 min and 15 sec for validation\n","\tTrain Loss: 0.221 | Train Acc: 91.29%\n","\t Val. Loss: 0.202 |  Val. Acc: 92.02%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"00sUvs8sD_3e","colab_type":"text"},"source":["# Inference on test set"]},{"cell_type":"code","metadata":{"id":"g_VuftmgECHo","colab_type":"code","colab":{}},"source":["# model.load_state_dict(torch.load('model.pt'))\n","# test_loss, test_acc, test_time = evaluate_model(model, test_iterator, criterion)\n","# print(f'Epoch {epoch} took {convert_time(test_time)} for test')\n","# print(f'\\t Test. Loss: {test_loss:.3f} |  Test. Acc: {test_acc*100:.2f}%')"],"execution_count":0,"outputs":[]}]}